# --- 1️⃣ Install dependencies ---
!pip install --upgrade pip
!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121  # change cu121 to cu118 if needed
!pip install git+https://github.com/QwenLM/Qwen-Image.git
!pip install diffusers transformers accelerate safetensors Pillow xformers gradio

# --- 2️⃣ Clone your repo (offline edits, examples, and UI) ---
!git clone https://github.com/Fanu2/offline-qwen-image-edit.git
%cd offline-qwen-image-edit

# --- 3️⃣ Import modules ---
import torch
from diffusers import QwenImageEditPipeline
import gradio as gr
from PIL import Image
import numpy as np
import random
import os
import json
import base64

# --- 4️⃣ Set device ---
device = "cuda" if torch.cuda.is_available() else "cpu"
dtype = torch.bfloat16 if device=="cuda" else torch.float32

# --- 5️⃣ Load Qwen Image Edit pipeline ---
pipe = QwenImageEditPipeline.from_pretrained(
    "Qwen/Qwen-Image-Edit", 
    torch_dtype=dtype
).to(device)

# --- 6️⃣ Simple inference function ---
MAX_SEED = np.iinfo(np.int32).max

def encode_image(pil_image):
    import io
    buffered = io.BytesIO()
    pil_image.save(buffered, format="PNG")
    return base64.b64encode(buffered.getvalue()).decode("utf-8")

def infer(image, prompt, seed=120, randomize_seed=False, num_inference_steps=50):
    if randomize_seed:
        seed = random.randint(0, MAX_SEED)
    generator = torch.Generator(device=device).manual_seed(seed)
    images = pipe(
        image,
        prompt=prompt,
        num_inference_steps=num_inference_steps,
        generator=generator,
        num_images_per_prompt=1
    ).images
    return images[0], seed

# --- 7️⃣ Launch Gradio UI ---
with gr.Blocks() as demo:
    input_image = gr.Image(label="Input Image", type="pil")
    prompt = gr.Textbox(label="Edit Prompt")
    seed_slider = gr.Slider(0, MAX_SEED, step=1, label="Seed", value=120)
    random_seed = gr.Checkbox(label="Randomize Seed", value=True)
    num_steps = gr.Slider(1, 50, step=1, label="Inference Steps", value=50)
    output_image = gr.Image(label="Edited Image", type="pil")
    run_button = gr.Button("Generate Edit")
    
    run_button.click(
        fn=infer,
        inputs=[input_image, prompt, seed_slider, random_seed, num_steps],
        outputs=[output_image, seed_slider]
    )

demo.launch()
